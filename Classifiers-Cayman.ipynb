{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_bmi_dataframe(meanbmi=False):\n",
    "    '''\n",
    "    returns a one-hot encoded dataframe where smoking status is calculated by a knn classifier and\n",
    "    bmi is calculated by knn regression if meanbmi is false, otherwise nans in the bmi column are \n",
    "    replaced by the mean column bmi.\n",
    "\n",
    "    returns Pandas DataFrame, Target Variable\n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "    #mix up the data since it came in ordered\n",
    "    df = df.sample(frac=1)\n",
    "    #drop the id column since it holds no predictive value as its not discrete\n",
    "    df = df.drop('id', axis=1)\n",
    "\n",
    "    #We now check to verify there are sufficient values in each category for the columns to run a prediction\n",
    "    categories = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "    #Since there is only one person with gender Other we drop the row because there are not enough for a prediction\n",
    "    df = df.drop(df[df['gender']=='Other'].index).reset_index(drop=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    '''\n",
    "    We leave the rows where people never worked since 22 is enough to gather some predictive information and\n",
    "    never working is very distinct from having a job. However, it may be linked to age, as there are some babies\n",
    "    in the dataset.\n",
    "\n",
    "    The smoking status column has the categories former smoker, never smoked, smokes, and unknown. The unknown values are\n",
    "    essencially Nan values. We are going to use a knn classifier to impute these values.\n",
    "\n",
    "    Before we do this, we need to one-hot encode the other categorical variables in order for the classifier to work, and\n",
    "    we need to drop the stroke column, as we don't want it to be used in this imputer because that would bias our final\n",
    "    predictions. We also need to encode the labels of the smoking_status column in order to be able to use the KNN classifier.\n",
    "    '''\n",
    "\n",
    "    le1 = LabelEncoder()\n",
    "    newcol = le1.fit_transform(df['smoking_status'])\n",
    "    df['smoking_status'] = pd.Series(newcol)\n",
    "    keys = le1.classes_\n",
    "    values = le1.transform(keys)\n",
    "    dictionary = dict(zip(keys, values))\n",
    "    inversedictionary = dict(zip(values, keys))\n",
    "\n",
    "    #one-hot encode the dataframe\n",
    "    df = pd.get_dummies(df, columns= ['gender', 'ever_married', 'work_type', 'Residence_type'])\n",
    "    y = df['stroke']\n",
    "    df = df.drop('stroke', axis=1)\n",
    "\n",
    "    #Find the best number of neighbors to use for the prediction\n",
    "    dfclass = df.drop('bmi', axis=1).dropna().copy()\n",
    "    dfclass['smoking_status'] = pd.Series(newcol)\n",
    "    dfclass = dfclass[dfclass['smoking_status'] != dictionary['Unknown']].copy()\n",
    "    yclass = dfclass['smoking_status'].copy()\n",
    "    dfclass = dfclass.drop('smoking_status', axis=1)\n",
    "\n",
    "    #Encode the labels in the prediction column\n",
    "    le = LabelEncoder().fit(yclass)\n",
    "    knnclass = KNeighborsClassifier()\n",
    "    parameters = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10,12,14,16,18,20,23,26,29]}\n",
    "    bestparamsclass = GridSearchCV(knnclass, parameters).fit(dfclass, yclass).best_params_\n",
    "\n",
    "\n",
    "    #Use the best value to fit a knnclassifier\n",
    "    df['smoking_status'] = df['smoking_status'].replace(to_replace=dictionary['Unknown'], value=np.nan)\n",
    "    yclassifier = df['smoking_status'].copy()\n",
    "    dfclassifier = df.drop(['bmi','smoking_status'], axis=1).copy()\n",
    "    knnclassifier = KNeighborsClassifier(n_neighbors=bestparamsclass['n_neighbors']).fit(dfclass, yclass)\n",
    "\n",
    "    #predict nan values only\n",
    "    for i in range(len(df)):\n",
    "        if np.isnan(yclassifier[i]):\n",
    "            yclassifier[i] = int(knnclassifier.predict(dfclassifier.loc[[i]]))\n",
    "\n",
    "    #replace all values with original or new predicted values\n",
    "    for i in range(len(yclassifier)):\n",
    "        yclassifier[i] = inversedictionary[yclassifier[i]]\n",
    "\n",
    "    #We next explore options to account for the Nan values in the BMI column\n",
    "    if meanbmi:\n",
    "\n",
    "        #We replace Nan values with the mean bmi\n",
    "        imputermean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        dfmean = pd.DataFrame(imputermean.fit_transform(df), columns=df.columns)\n",
    "        dfmean['smoking_status'] = yclassifier\n",
    "        dfmean = pd.get_dummies(dfmean, columns=['smoking_status'])\n",
    "        return dfmean, y\n",
    "\n",
    "    else:\n",
    "        #We use knn to fill in the missing values\n",
    "        #We begin by finding the optimal number of neighbors to use for the prediction\n",
    "        dfreg = df.dropna()\n",
    "        yreg = dfreg['bmi']\n",
    "        dfreg = dfreg.drop(['bmi','smoking_status'], axis=1)\n",
    "        knnreg = KNeighborsRegressor()\n",
    "        parameters = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10,12,14,16,18,20,23,26,29]}\n",
    "        bestparams = GridSearchCV(knnreg, parameters).fit(dfreg, yreg).best_params_\n",
    "\n",
    "        #Use the best value to impute the values to use for bmi\n",
    "        imputerKNN = KNNImputer(n_neighbors=bestparams['n_neighbors'])\n",
    "        dfKNN = pd.DataFrame(imputerKNN.fit_transform(df), columns = df.columns)\n",
    "        dfKNN['smoking_status'] = yclassifier\n",
    "        dfKNN = pd.get_dummies(dfKNN, columns=['smoking_status'])\n",
    "        return dfKNN, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "data,target = cont_bmi_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Gaussian/Bernoulli w/o SMOTE:\n",
      "Recall: 0.044444444444444446\n",
      "Accuracy: 0.9354207436399217\n",
      "F1 Score: 0.07476635514018692\n",
      "\n",
      "Bernoulli (just categorical) w/o SMOTE:\n",
      "Recall: 0.05555555555555555\n",
      "Accuracy: 0.928897586431833\n",
      "F1 Score: 0.08403361344537814\n",
      "\n",
      "Gaussian (just continuous) w/o SMOTE:\n",
      "Recall: 0.14444444444444443\n",
      "Accuracy: 0.9080234833659491\n",
      "F1 Score: 0.155688622754491\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes w/o SMOTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3)\n",
    "\n",
    "bernoulliSet = X_train.drop(columns=[\"avg_glucose_level\", \"bmi\", \"age\"])\n",
    "gaussianSet = X_train[[\"avg_glucose_level\", \"bmi\", \"age\"]]\n",
    "\n",
    "bernoulliClassifier = BernoulliNB()\n",
    "gaussianClassifier = GaussianNB()\n",
    "\n",
    "bernoulliTest = X_test.drop(columns=[\"avg_glucose_level\", \"bmi\", \"age\"])\n",
    "gaussianTest = X_test[[\"avg_glucose_level\", \"bmi\", \"age\"]]\n",
    "\n",
    "bernoulliProb = bernoulliClassifier.fit(bernoulliSet, y_train).predict_proba(bernoulliTest)\n",
    "gaussianProb = gaussianClassifier.fit(gaussianSet, y_train).predict_proba(gaussianTest)\n",
    "prediction = (np.argmax(bernoulliProb*gaussianProb, axis = 1))\n",
    "\n",
    "print(\"Combined Gaussian/Bernoulli w/o SMOTE:\")\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))\n",
    "print()\n",
    "\n",
    "prediction = bernoulliClassifier.fit(bernoulliSet, y_train).predict(bernoulliTest)\n",
    "\n",
    "print(\"Bernoulli (just categorical) w/o SMOTE:\")\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))\n",
    "print()\n",
    "\n",
    "prediction = gaussianClassifier.fit(gaussianSet, y_train).predict(gaussianTest)\n",
    "\n",
    "print(\"Gaussian (just continuous) w/o SMOTE:\")\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Gaussian/Bernoulli w/ SMOTE:\n",
      "Recall: 0.5952380952380952\n",
      "Accuracy: 0.776255707762557\n",
      "F1 Score: 0.22573363431151242\n",
      "\n",
      "Bernoulli (just categorical) w/ SMOTE:\n",
      "Recall: 0.40476190476190477\n",
      "Accuracy: 0.8114807566862361\n",
      "F1 Score: 0.1904761904761905\n",
      "\n",
      "Gaussian (just continuous) w/ SMOTE:\n",
      "Recall: 0.8214285714285714\n",
      "Accuracy: 0.6797129810828441\n",
      "F1 Score: 0.21939586645469\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes w/ SMOTE\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3)\n",
    "\n",
    "X_train, y_train = sm.fit_resample(X_train,y_train)\n",
    "\n",
    "bernoulliSet = X_train.drop(columns=[\"avg_glucose_level\", \"bmi\", \"age\"])\n",
    "gaussianSet = X_train[[\"avg_glucose_level\", \"bmi\", \"age\"]]\n",
    "\n",
    "bernoulliClassifier = BernoulliNB()\n",
    "gaussianClassifier = GaussianNB()\n",
    "\n",
    "bernoulliTest = X_test.drop(columns=[\"avg_glucose_level\", \"bmi\", \"age\"])\n",
    "gaussianTest = X_test[[\"avg_glucose_level\", \"bmi\", \"age\"]]\n",
    "\n",
    "bernoulliProb = bernoulliClassifier.fit(bernoulliSet, y_train).predict_proba(bernoulliTest)\n",
    "gaussianProb = gaussianClassifier.fit(gaussianSet, y_train).predict_proba(gaussianTest)\n",
    "prediction = (np.argmax(bernoulliProb*gaussianProb, axis = 1))\n",
    "\n",
    "#prediction = bernoulliClassifier.fit(bernoulliSet, y_train).predict(bernoulliTest)\n",
    "#prediction = gaussianClassifier.fit(gaussianSet, y_train).predict(gaussianTest)\n",
    "\n",
    "print(\"Combined Gaussian/Bernoulli w/ SMOTE:\")\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))\n",
    "print()\n",
    "\n",
    "prediction = bernoulliClassifier.fit(bernoulliSet, y_train).predict(bernoulliTest)\n",
    "\n",
    "print(\"Bernoulli (just categorical) w/ SMOTE:\")\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))\n",
    "print()\n",
    "\n",
    "prediction = gaussianClassifier.fit(gaussianSet, y_train).predict(gaussianTest)\n",
    "\n",
    "print(\"Gaussian (just continuous) w/ SMOTE:\")\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Gaussian/Bernoulli w/ SMOTE (Cross-Validated 7-Fold):\n",
      "Recall mean: 0.651331924603814\n",
      "Accuracy mean: 0.7948708656471214\n",
      "F1 Score mean: 0.23541575539480236\n",
      "\n",
      "Bernoulli w/ SMOTE (Cross-Validated 7-Fold):\n",
      "Recall mean: 0.37933502749170955\n",
      "Accuracy mean: 0.8404800828951007\n",
      "F1 Score mean: 0.18759931790199497\n",
      "\n",
      "Gaussian w/ SMOTE (Cross-Validated 7-Fold):\n",
      "Recall mean: 0.833274636270028\n",
      "Accuracy mean: 0.6979834585618451\n",
      "F1 Score mean: 0.21109612339930808\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes w/ SMOTE Cross Validated (7-Fold)\n",
    "\n",
    "kf = KFold(n_splits = 7)\n",
    "\n",
    "recallScoresCombined = []\n",
    "accuracyScoresCombined = []\n",
    "f1ScoresCombined = []\n",
    "\n",
    "recallScoresGaussian = []\n",
    "accuracyScoresGaussian = []\n",
    "f1ScoresGaussian = []\n",
    "\n",
    "recallScoresBernoulli = []\n",
    "accuracyScoresBernoulli = []\n",
    "f1ScoresBernoulli = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data), 1):\n",
    "    X_train = data.iloc[train_index]\n",
    "    y_train = target.iloc[train_index]\n",
    "    X_test = data.iloc[test_index]\n",
    "    y_test = target.iloc[test_index]\n",
    "\n",
    "    sm = SMOTE()\n",
    "    X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    bernoulliSet = X_train_oversampled.drop(columns=[\"avg_glucose_level\", \"bmi\", \"age\"])\n",
    "    gaussianSet = X_train_oversampled[[\"avg_glucose_level\", \"bmi\", \"age\"]]\n",
    "\n",
    "    bernoulliClassifier = BernoulliNB()\n",
    "    gaussianClassifier = GaussianNB()\n",
    "\n",
    "    bernoulliTest = X_test.drop(columns=[\"avg_glucose_level\", \"bmi\", \"age\"])\n",
    "    gaussianTest = X_test[[\"avg_glucose_level\", \"bmi\", \"age\"]]\n",
    "\n",
    "    bernoulliProb = bernoulliClassifier.fit(bernoulliSet, y_train_oversampled).predict_proba(bernoulliTest)\n",
    "    gaussianProb = gaussianClassifier.fit(gaussianSet, y_train_oversampled).predict_proba(gaussianTest)\n",
    "    prediction = (np.argmax(bernoulliProb*gaussianProb, axis = 1))\n",
    "\n",
    "    recallScoresCombined.append(recall_score(y_test, prediction))\n",
    "    accuracyScoresCombined.append(accuracy_score(y_test, prediction))\n",
    "    f1ScoresCombined.append(f1_score(y_test, prediction))\n",
    "\n",
    "    prediction = bernoulliClassifier.fit(bernoulliSet, y_train_oversampled).predict(bernoulliTest)\n",
    "    \n",
    "    recallScoresBernoulli.append(recall_score(y_test, prediction))\n",
    "    accuracyScoresBernoulli.append(accuracy_score(y_test, prediction))\n",
    "    f1ScoresBernoulli.append(f1_score(y_test, prediction))\n",
    "    \n",
    "    prediction = gaussianClassifier.fit(gaussianSet, y_train_oversampled).predict(gaussianTest)\n",
    "\n",
    "    recallScoresGaussian.append(recall_score(y_test, prediction))\n",
    "    accuracyScoresGaussian.append(accuracy_score(y_test, prediction))\n",
    "    f1ScoresGaussian.append(f1_score(y_test, prediction))\n",
    "\n",
    "print(\"Combined Gaussian/Bernoulli w/ SMOTE (Cross-Validated 7-Fold):\")\n",
    "print(\"Recall mean:\", np.mean(recallScoresCombined))\n",
    "print(\"Accuracy mean:\", np.mean(accuracyScoresCombined))\n",
    "print(\"F1 Score mean:\", np.mean(f1ScoresCombined))\n",
    "print()\n",
    "\n",
    "print(\"Bernoulli w/ SMOTE (Cross-Validated 7-Fold):\")\n",
    "print(\"Recall mean:\", np.mean(recallScoresBernoulli))\n",
    "print(\"Accuracy mean:\", np.mean(accuracyScoresBernoulli))\n",
    "print(\"F1 Score mean:\", np.mean(f1ScoresBernoulli))\n",
    "print()\n",
    "\n",
    "print(\"Gaussian w/ SMOTE (Cross-Validated 7-Fold):\")\n",
    "print(\"Recall mean:\", np.mean(recallScoresGaussian))\n",
    "print(\"Accuracy mean:\", np.mean(accuracyScoresGaussian))\n",
    "print(\"F1 Score mean:\", np.mean(f1ScoresGaussian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target = cont_bmi_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier w/o SMOTE:\n",
      "Recall mean: 0.02\n",
      "Accuracy mean: 0.9436299549001305\n",
      "F1 Score mean: 0.029968066814050603\n",
      "\n",
      "KNN Classifier w/ SMOTE:\n",
      "Recall mean: 0.5141224489795919\n",
      "Accuracy mean: 0.7929164646149068\n",
      "F1 Score mean: 0.2039133111623574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest Neighbors (No Parameter Tuning/Cross Validated)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print(\"KNN Classifier w/o SMOTE:\")\n",
    "print(\"Recall mean:\", np.mean(cross_validate(knn, data, target, scoring=\"recall\")[\"test_score\"]))\n",
    "print(\"Accuracy mean:\", np.mean(cross_validate(knn, data, target, scoring=\"accuracy\")[\"test_score\"]))\n",
    "print(\"F1 Score mean:\", np.mean(cross_validate(knn, data, target, scoring=\"f1\")[\"test_score\"]))\n",
    "print()\n",
    "\n",
    "model = Pipeline([('sampling', SMOTE()),('classification', KNeighborsClassifier())])\n",
    "\n",
    "print(\"KNN Classifier w/ SMOTE:\")\n",
    "print(\"Recall mean:\", np.mean(cross_validate(model, data, target, scoring=\"recall\")[\"test_score\"]))\n",
    "print(\"Accuracy mean:\", np.mean(cross_validate(model, data, target, scoring=\"accuracy\")[\"test_score\"]))\n",
    "print(\"F1 Score mean:\", np.mean(cross_validate(model, data, target, scoring=\"f1\")[\"test_score\"]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier w/o SMOTE and parameter tuning via GridSearch:\n",
      "Recall mean: 0.10840816326530611\n",
      "Accuracy mean: 0.9121186971830311\n",
      "F1 Score mean: 0.10461931919376691\n",
      "\n",
      "KNN Classifier w/ SMOTE and parameter tuning via GridSearch:\n",
      "Recall mean: 0.5944489795918366\n",
      "Accuracy mean: 0.779803960278381\n",
      "F1 Score mean: 0.2092442818078263\n"
     ]
    }
   ],
   "source": [
    "#KNN Neighbors Classifier (parameter tuning through gridSearch)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {'n_neighbors':[2,3,4,5,6,7,8,9,10], \"weights\":[\"uniform\", \"distance\"]}\n",
    "gridSearch = GridSearchCV(knn, parameters, scoring=\"f1\")\n",
    "gridSearch.fit(data, target)\n",
    "\n",
    "print(\"KNN Classifier w/o SMOTE and parameter tuning via GridSearch:\")\n",
    "print(\"Recall mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"recall\")[\"test_score\"]))\n",
    "print(\"Accuracy mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"accuracy\")[\"test_score\"]))\n",
    "print(\"F1 Score mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"f1\")[\"test_score\"]))\n",
    "print()\n",
    "\n",
    "model = Pipeline([('sampling', SMOTE()),('classification', KNeighborsClassifier())])\n",
    "\n",
    "parameters = {'classification__n_neighbors':[2,3,4,5,6,7,8,9,10], \"classification__weights\":[\"uniform\", \"distance\"]}\n",
    "gridSearch = GridSearchCV(model, parameters, scoring=\"f1\")\n",
    "gridSearch.fit(data,target)\n",
    "\n",
    "print(\"KNN Classifier w/ SMOTE and parameter tuning via GridSearch:\")\n",
    "print(\"Recall mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"recall\")[\"test_score\"]))\n",
    "print(\"Accuracy mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"accuracy\")[\"test_score\"]))\n",
    "print(\"F1 Score mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"f1\")[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier w/o SMOTE and parameter tuning via GridSearch with Standard Scalar Preprocessing:\n",
      "Recall mean: 0.11632653061224489\n",
      "Accuracy mean: 0.9132896070963772\n",
      "F1 Score mean: 0.11556957895102224\n",
      "\n",
      "KNN Classifier w/ SMOTE and parameter tuning via GridSearch with Standard Scalar Preprocessing:\n",
      "Recall mean: 0.2529795918367347\n",
      "Accuracy mean: 0.8833431404306051\n",
      "F1 Score mean: 0.1767126099888079\n"
     ]
    }
   ],
   "source": [
    "#KNN Neighbors Classifier (parameter tuning through gridSearch/Standard Scalar Preprocessing)\n",
    "\n",
    "model = Pipeline([('scale', StandardScaler()),('classification', KNeighborsClassifier())])\n",
    "\n",
    "parameters = {'classification__n_neighbors':[2,3,4,5,6,7,8,9,10], \"classification__weights\":[\"uniform\", \"distance\"]}\n",
    "gridSearch = GridSearchCV(model, parameters, scoring=\"f1\")\n",
    "gridSearch.fit(data, target)\n",
    "\n",
    "print(\"KNN Classifier w/o SMOTE and parameter tuning via GridSearch with Standard Scalar Preprocessing:\")\n",
    "print(\"Recall mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"recall\")[\"test_score\"]))\n",
    "print(\"Accuracy mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"accuracy\")[\"test_score\"]))\n",
    "print(\"F1 Score mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"f1\")[\"test_score\"]))\n",
    "print()\n",
    "\n",
    "model = Pipeline([('sampling', SMOTE()),('scale', StandardScaler()),('classification', KNeighborsClassifier())])\n",
    "\n",
    "parameters = {'classification__n_neighbors':[2,3,4,5,6,7,8,9,10], \"classification__weights\":[\"uniform\", \"distance\"]}\n",
    "gridSearch = GridSearchCV(model, parameters, scoring=\"f1\")\n",
    "gridSearch.fit(data,target)\n",
    "\n",
    "print(\"KNN Classifier w/ SMOTE and parameter tuning via GridSearch with Standard Scalar Preprocessing:\")\n",
    "print(\"Recall mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"recall\")[\"test_score\"]))\n",
    "print(\"Accuracy mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"accuracy\")[\"test_score\"]))\n",
    "print(\"F1 Score mean:\", np.mean(cross_validate(gridSearch.best_estimator_, data, target, scoring=\"f1\")[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Learning Algorithms:\n",
    "\n",
    "We tried a variety of classifiers in an attempt to create a model that could accurately (and more importantly, with high recall) predict stroke occurances using the health data provided. One important characteristic of our dataset is that it is heavily imbalanced. The null hypothesis (predicting no stroke occurances across the board) results in a model with about 95% accuracy. Thus, it is extremely important that we focus on the recall of the model and the f1 scores, which represent a sort of balance between the accuracy and the recall scores. Experimentally, raising a model's recall by changing model parameters or using SMOTE to oversample the training data results in a lowering of the accuracy so there is a practical trade-off between the two scores. However, the null hypothesis model is quite obviously useless so it's a trade-off we need to make in order to get any kind of applicable model.\n",
    "\n",
    "Treating the natural imbalance in the data is of utmost importance. The Syntethic Minority Oversampling Technique (SMOTE) is used to take the minority class (in this case, stroke occurances) and creating fake examples that are close to the other stroke occurances in the feature space. In all of our attempted models, oversampling with SMOTE greatly improves both recall and the f1 score from the corresponding baseline model that was trained only on the provided data. Regardless of the classifier type, the models were effectively useless without oversampling. Thus, the most important step in model creation for our dataset is using SMOTE in our pipeline. We also played around with other preprocessing steps such as using sklearn's StandardScaler, but this reduces the scoring of our models across the board.\n",
    "\n",
    "Only two of the provided features are continuous/numerical, so Gaussian Discriminant Analysis was not an appropriate tool to use due to the fact that it assumes that the features are normally distributed. Instead, we turned to Naive Bayes as a potential model because it can handle both Gaussian and Binary distributed features. Due to the fact that Naive Bayes assumes independence between the features, we can split the dataset into Gaussian and Bernoulli features and get final probabilities at the end by multiplying the probabilities of the two separate models together. For each of the sets, we can train a Naive Bayes model of the corresponding type on just the appropriate features. Then, rather than using the predict method to get labels, we grab the probabilities for each class. Multiplying these newly calculated probabilities together from both the Gaussian and Bernoulli models, we get a \"combined\" probability that accounts for all of the features in the data. Then, assigning class labels is as easy as choosing the argmax of the resultant probabilities for each input. In addition to being theoretically sound (by the assumed independence of features), we experimentally see that the combined model improves on the f1 scores of both subset models. Using this combined model (w/ SMOTE), we end up with a recall of $0.65$, an accuracy of $0.79$, and an f1 of around $0.24$. Unfortunately, there aren't a whole lot of model parameters that can be changed for the Naive Bayes model. The only possiblity being \"var_smoothing\" which is just used for calculation stability on GaussianNB and \"alpha\" a smoothing parameter for BernoulliNB. Thus, the best we can do with Naive Bayes is an f1 score of $0.24$. \n",
    "\n",
    "Another option we explored as a way to predict stroke occurances was a K-Nearest Neighbors Classifier. Without SMOTE or any fine tuning of parameters, the model is useless with a recall score around $.01$ and an f1 score of about $0.02$. Oversampling immediately yields much better results with a recall score of about $0.51$, an accuracy of $0.79$, and an f1 score of around $0.19$ which is actually already decent compared to our scores on the Naive Bayes. Unfortunately, the tuning of model parameters using GridSearch doesn't improve the SMOTE model significantly. It provides a slight jump up to a recall of $.59$, a slight accuracy decrease to $.78$, and a slight jump up to a f1 score of about $.21$. Thus, even with all the improvements we could make with our KNN classifier, it underperforms when compared to the combined Naive Bayes model we talked about previously."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4a22ef55f1677bb95c5c32c0ec0f93ada970bc391369c7e693145400d5c0908"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
